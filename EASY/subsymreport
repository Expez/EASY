This report contains an overview of EASY, Evolutionary Algorithm SYstem.  EASY is written in
Java, with extensive use of generics to create a system which is applicable to as many types of
problems as possible. We then use EASY to solve the One Max Fitness problem and the Blotto problem.

System Overview

The most natural starting point, when describing our system, is by describing the Individual
class. We're using Java and have chosen to make the Individual class into a generic class. Generics
are used in order to have a re-usable Individual.  GType refers to the genotype of the individual,
and PType refers to the phenotype.

Thus, at the source code level you will see Individual<GType, PType>. In the case of the One Max
problem both GType and PType will be integer arrays, because both the genotype and the phenotype of
these individuals are bitvectors.

Individual<GType, PType> is actually an interface, which all problem specific individuals must
adhere to. E.g the BitvectorIndividual<int[], int[]> class used to solve the one-max fitness
problem. Adhering to the Individual interface means implementing the following methods:
getFitness(), getGenome(), getPhenome(), updateFitness(FitnessCalculator fitCalc) and growUp().

The first three are self-explanatory, the return information from these function calls is used to
generate the report, showing the results of the evolutionary algorithm.

The updatefitness(FitnessCalculator fitcalc) method takes a problem specific class, fitCalc, of type
FitnessCalculator. The FitnessCalculator classes -- classes implementing the FitnessCalculator
interface -- are used to encapsulate the problem specific information about how to calculate the
fitness of each individual. By encapsulating the information in this way we avoid the need for
problem specific details in the Individual interface.

The growUp() method is called in order to create the phenome of the individual.

Individuals are aggregated using the Population class. This class is a wrapper around a list of
individuals. It exposes quite a few methods from the java.util.List interface, but also a few other
methods: like shuffle(), which shuffles the sequential ordering of the individuals, and drop(n)
which drops n individuals from the start of the sequence of individuals. A sort() method is also
available to sort the population, on fitness value, in ascending or descending order.

A few utility methods can also be found in Population. Examples are updateFitness() which updates
the fitness for every individual in the population, and getRandomPopulation() which is a static
method used to instantiate a new population. This method was added to make life easier when writing
unit tests.

Interfaces are defined for the adult selection mechanisms, AdultSelector, and for the parent
selection mechanisms, ParentSelector. The AdultSelector and ParentSelector interfaces both require a
single method to be implemented, select().  In the case of AdultSelector select(Population
population) uses the currently active parent selection mechanism in order to select parents. In the
case of AdultSelector select(Population adults, Population children) combines the Population
containing the adults and childrens, using the currently active adult selection mechanism, in order
to return a single Population containing the individuals who are to become adults in the next
generation.

If you now ask how babies are made, the answer is in the Incubator. The classes implementing the
Incubator interface implement two methods: makeChildren(parents) and getRandomIndividual().
makeChildren(parents) takes a Population of individuals, the ones selected to become parents, and
returns a Population of children.

getRandomIndividual() does exactly what the name implies, it instantiates a new individual, with a
randomized genome (and phenome). This method is used when setting up the initial Population, before
the evolutionary loop can start.

In order to do their work the Incubator classes all contain another class implementing the
Replicator interface. The Replicator interface defines the following methods: mutate(genome),
combine(genome1, genome2)and randomGenome(). mutate(genome) mutates the genome passed to it and
returns it. combine(g1, g2) combines genomes g1 and g2 and returns a new genome, currently this is
where crossover happens, but if we want to use some other method for combining genomes, the system
could easily support it. randomGenome() returns a new random genome and thus aids in setting up the
initial population of random individuals.

The Main class contains an Evolution class. In order to run the evolutionary loop a call is made to
runEvolution(Environment env). Environment is a data class containing the evolutionary parameters:
crossover rate, mutation rate, etc. The main evolutionary loop is then: 
  updateFitness()
  selectParents() 
  createChildren() 
  selectAdults()

We're using GNUPlot in order to do the plotting, and have made an Output class to write the results
to file. When the run completes a call is made to GNUPlot to plot the contents in the output file.
We've also made a Report interface in order to compile a more extensive report. The Report interface
defines two methods log(population, generation) and writeToStream(stream). The log method does
whatever logging is desired with access to the current Population and the generation number. The
writeToStream(stream) methods tells the Report class where to write the report, the most used
parameters will be stdout, to write to standard out, or a file stream, in order to write the report
to file.

The only part of the system which is not completely general, through the use of generics, are the
problem specific classes implementing the following interfaces: FitnessCalculator, Incubator and
Replicatorm, and Individual. Thankfully, a lot of problems rely on the same representation, e.g a
bitvector. Because of this we can re-use our BitvectorIncubator and our BitvectorReplicator. When
we encounter another such problem using a bitvector representation all we have to do is extend the
abstract Individual class, containing 90% of the code if the BitvectorIndividual, and then
re-implement the problem specific growUp() method (which performs the problem specific mapping from
genome to phenome).

Right now we're quite happy with the overall structure of the system, but we could've used a bit
more time to refactor. Thankfully exercise two will use the same system, so we will reap the benefit
of the planned refactorings. One of these planned refactorings is to avoid passing a FitnessCalculator
to the Population class. Removing responsibilities of setting fitness from the Population class would
greatly improve cohesion. 

selectAdults() actually does some work in determining whether a fitness update is actually
necessary or not. If full generational replacement is used, no fitness update is needed.  The reason
we're not calling updateFitness() on the newly created children is because in the case of the Blotto
problem fitness can only be determined if the entire population, of adults and children, is
considered as a whole. The same happens when overproduction is used, then we need to update the
fitness values of the children in order to select the adults to enter the next generation. 
This is something we would like to improve, it feels a bit clumsy right now; we're breaking the level
of abstraction by updating fitness in the runEvolution function  as well as in some of the functions
this function calls.

We're probably going to implement our own plotting routine, so plotting can be made available by simply
bundling some .jar file with the program. This makes it possibly to provide the plotting feature 
without requiring the user to install GNUPlot. Plenty of plotting libraries exist, so even though
GNUPlot is pretty awesome, this makes a more self-contained system.

Analysis of EASY on the 40-bit One-Max problem

The impact of crossover and mutation rate.

We're were able to find the correct solution using a crossover rate and mutation rate of 1% with a
population size of of 60. The adult selection mechanism was full generational replacement and the
parent selection mechanism was fitness proportionate selection. Elitism, ensuring the survival of
the 3 best individuals, was also used.  Population as low as half that would consistently reach 0.9
and above, but it took quite a few extra generations in order to consistently get the correct solution.

PIC: Onemax1 shows initial run.

PIC: Onemax2 shows mutation rate changed to 0.1

In the picture above we've changed the mutation rate to to 0.1, an increase by a factor ten.
This large increase was clearly detrimental for the evolutionary performance. A large mutation rate
means that we're making large jumps in the search space each generation. A better search strategy is
to move a small direction in search space, and only continue moving in that direction if the move
resulted in a better solution. The first strategy is more akin to random search, and the latter
describes basic hill climbing through the search space.

PIC: Onemax3 shows crossover rate changed to 0.1

In the picture above the crossover rate has been changed to 0.1, again an increase by a factor ten.
Surpisingly -- at least initially! -- the fitness plot isn't as radically different from the first
fitness plot as one would expect. The reason this shouldn't come as such a huge surprise is because
the crossover operator is in essence copying a sub-solution from one individual to another. In the
case of the one max fitness problems all the sub-solutions are so similar, that the crossover
operator doesn't amount to much. Initially the crossover impact might be pretty large, because
sub-solutions from the best individuals are traded, but at the end we've accumulated a lot of 1's in
the one-max genomes and crossover a large part of the genome might mean flipping a single bit.

We ran several runs with different values for crossover and the effect it has on the one max problem
seems limited, but positive. The effects of changing the mutation rate, on the other hand, were
quickly noticed.  It's hard to say exactly which value for mutation rate is the best one, but the
area around 0.01 seems most promising.

The impact of parent selection mechanism

PIC: Onemax4 shows rank based selection.

Rank based selection performed quite a bit better than the fitness proportionate selection. The rank
parameter used was equal to half the population size. In other words half of the adults are allowed
to procreate. Fitness proportionate selection managed to arrive at the solution in 73 generations,
whereas rank based selection needed only 31. This is quite the improvement! Rank based selection
amounts to higher selection pressure, the worst individuals are completely excluded from reproducing. On the other hand, the exploration added, by only removing half of the population seems to have increased performance by a noticable amount. The likelihood of selecting an individual with truly abysmal fitness can be very small when using fitness proportionate selection, but using rank based selection individuals with low fitness still get to reproduce. Because we don't exclude already selected individuals from entering the parent list again, fitness proportionate selection probably doesn't do much exploration at all.

PIC: Onemax5 shows tournament based selection.

Tournament selection, with a tournament size of 5. We created a number of parents equal to
approximately half the population. The results is a very similar performance as what was found with
rank based selection. The explanation is also the same. This run shows a solution found in 43
generations.


PIC: Onemax6 shows sigmascaled selection

Clearly we have our winner! This performance is pretty outrageous compared to what we got initially. Again we're creating a number of adults equal to approximately half the population size. The solution is found in only 15 generations. The sigma scaled selection amounts to even more exploration, compared to the rank based selection. This is because the scaled fitness values reduces the difference between the likelihood of reproduction for individuals with very low and very high fitness. 

Modification of the target string

Modification of the target string had no discernable effect on the performance of the system.  The
reason for this is how we created our crossover operator. We don't, currently, have any wrap-around.
What this means is that the crossover operator creates a new genome by taking the first genome up to
the randomly selected crossover point and then the second genome from then on out. If we used
wrap-around we would notice a deleterious effect on the system when we changed the target
string. The reason being that we're preserving sub-solutions by not having wrap around. An example
might be in order here. Imagine we have the target vector:
T = [0, 0, 1, 1]. 
We also have two individuals 
I1 = [1,0 1,1] and
I2 = [0,0,1,0].

Both I1 and I2 are one bit away from the solution. Now imagine that the we randomly pick a crossover point and select that crossover point to cross over at the end of I2 and that the length of the section to cross over is randomly selected to be of length 2. The resulting individual, using wrap-around, will have the end of I1 combined with the end of I2:
I3 = [1,1,1,0] this individual has 3 bits wrong and is significantly worse than either of its parents!


The Blotto problem

The genotype used to solve the Blotto problem is a normalized double array. The mapping to phenotype
is 1:1 and each entry in the double array of the phenotype indicates the proportion of available
troops which are used for the battle in question. A genotype might be [0.75, 0.25, 0] which maps to
the phenotype [0.75, 0.25, 0] which means the general is deploying 75% of his troops to the first
battle, 25%, to the second battle and 0% to the final battle.

The importance of entropy

The entropy function returns high values for balanced strategies. We say that a strategy is balanced
if a significant portion of resources is diverted to each battle. A strategy with a lot of battles
with no resources spent will have very low entropy.

The plot where Rf = Lf = 1 quickly converges on a strategy with very low entropy. The reason for
this is that if you lose the first battle, all your remaining soldiers will defect and you will lose
all your remaining battles. The optimal strategy here is to use all your soldiers in the first
battle, this strategy has an entropy of 0.

In the plot where Rf = Lf = 0 we can see that the entropy fluctuates a lot more. This is because
when the rival strategies have very high entropy a good counter strategy will be one with
comparatively low entropy, and vice versa. In more concrete terms we can say that a low entropy
strategy spends a lot of resources on a few battles, whereas a low entropy strategy spreads its
resources (perhaps too thinly!) among all battles.

The three signature cases

B = 20 ,Rf = 0.5, Lf = 0.5

PIC: 20-0.5-0.5-fitness
PIC: 20-0.5-0.5-entropy

This case is interesting because we can see that the entropy is clearly oscillating. Some unbalanced
strategy will emerge (low entopy), and win a few key battles while neglecting the rest, and then a
more balanced counter-strategy (with high entropy) will emerge to beat it. The max fitness plot is
also oscillating wildly, showing that new winning strategies are constantly re-appearing and being
countered. Notice how the average fitness always stays the same in all the plots for the Blotto
problem. This is because we're dealing with a zero-sum game: a battle won for one strategy always
means a battle lost for another strategy.

B = 5, Rf = 0.5, Lf = 0.

PIC: 5-0.5-0.0-fitness
PIC: 5-0.5-0.0-entropy

For this case we see a huge drop-off in entropy down to nearly zero, then the system stays there for
some time while this strategy dominates. I checked the log and this strategy is, naturally, [0.998, 0.000, 0.000, 0.002, 0.000]. The benefit of being able to redeploy soldiers plays a huge role here.
The following section in the entropy graph is also quite interesting. The obvious counter strategy to a strategy with no troops deployed to fight most of the battles is to send a single unit there to win the battle unopposed. Here is the winning strategy at generation 445: [0.903, 0.000, 0.000, 0.097, 0.000]. Redeployment ensures that a small fraction is sent to fight in battles 2 and 3, what is probably two units is deployed to battle 4 and a single unit is redeployed to battle 5. This is pretty
cool! 

B = 20, Rf = 1.0, Lf = 1.0.

PIC: 20-1.0-1.0-fitness
PIC: 20-1.0-1.0-entropy

This case is interesting because it clearly shows the behaviour we get at the extreme. A high value
for Rf and a high value for Lf both favor strategies where a lot of resources are diverted to the
early battles. This explains the sudden drop-off in entropy; we're producing strategies with some
high value for the first battle, and perhaps the second one, and then all zeroes. The reason the
entropy stays at zero, and never oscillates up, is because Lf is 1. If you lose the first battle,
you will lose all your remaining battles because all your troops will defect! Clearly, very high
values for Lf will produce somewhat uninteresting cases.

Co-evolution

Co-evolution is interesting because of the oscillating patterns seen in the fitness plots and in the entropy plots. Because the fitness of each individual depends on what the other individuals are doing we will always see new strategies appear and disappear. The only exception here is if impose some parameters which forces some, often obvious, strategies to appear. An example we saw of this is when Lf was is to 1. The winningest strategy in this case is to use all the resources to win the first battle.
